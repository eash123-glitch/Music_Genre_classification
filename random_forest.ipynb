{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/20 14:37:39 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/04/20 14:37:39 ERROR Configuration: error parsing conf core-default.xml\n",
      "java.nio.file.NoSuchFileException: /home/eashwar123/Desktop/sem4/Bigdata/project/venv/lib/python3.10/site-packages/pyspark/jars/hadoop-client-api-3.3.4.jar\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)\n",
      "\tat java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)\n",
      "\tat java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)\n",
      "\tat java.base/java.nio.file.Files.readAttributes(Files.java:1764)\n",
      "\tat java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1414)\n",
      "\tat java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)\n",
      "\tat java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)\n",
      "\tat java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)\n",
      "\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)\n",
      "\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)\n",
      "\tat java.base/java.util.jar.JarFile.<init>(JarFile.java:348)\n",
      "\tat java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)\n",
      "\tat java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)\n",
      "\tat java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)\n",
      "\tat java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)\n",
      "\tat java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)\n",
      "\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)\n",
      "\tat org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)\n",
      "\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)\n",
      "\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1412)\n",
      "\tat org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendSparkHadoopConfigs(SparkHadoopUtil.scala:512)\n",
      "\tat org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendS3AndSparkHadoopHiveConfigurations(SparkHadoopUtil.scala:443)\n",
      "\tat org.apache.spark.deploy.SparkHadoopUtil$.newConfiguration(SparkHadoopUtil.scala:428)\n",
      "\tat org.apache.spark.deploy.SparkHadoopUtil.newConfiguration(SparkHadoopUtil.scala:122)\n",
      "\tat org.apache.spark.SecurityManager.<init>(SecurityManager.scala:99)\n",
      "\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:262)\n",
      "\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:196)\n",
      "\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:284)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:483)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/04/20 14:37:39 ERROR SparkContext: Error initializing SparkContext.\n",
      "java.lang.RuntimeException: java.nio.file.NoSuchFileException: /home/eashwar123/Desktop/sem4/Bigdata/project/venv/lib/python3.10/site-packages/pyspark/jars/hadoop-client-api-3.3.4.jar\n",
      "\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3089)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)\n",
      "\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)\n",
      "\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1412)\n",
      "\tat org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendSparkHadoopConfigs(SparkHadoopUtil.scala:512)\n",
      "\tat org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendS3AndSparkHadoopHiveConfigurations(SparkHadoopUtil.scala:443)\n",
      "\tat org.apache.spark.deploy.SparkHadoopUtil$.newConfiguration(SparkHadoopUtil.scala:428)\n",
      "\tat org.apache.spark.deploy.SparkHadoopUtil.newConfiguration(SparkHadoopUtil.scala:122)\n",
      "\tat org.apache.spark.SecurityManager.<init>(SecurityManager.scala:99)\n",
      "\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:262)\n",
      "\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:196)\n",
      "\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:284)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:483)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.nio.file.NoSuchFileException: /home/eashwar123/Desktop/sem4/Bigdata/project/venv/lib/python3.10/site-packages/pyspark/jars/hadoop-client-api-3.3.4.jar\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)\n",
      "\tat java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)\n",
      "\tat java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)\n",
      "\tat java.base/java.nio.file.Files.readAttributes(Files.java:1764)\n",
      "\tat java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1414)\n",
      "\tat java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)\n",
      "\tat java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)\n",
      "\tat java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)\n",
      "\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)\n",
      "\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)\n",
      "\tat java.base/java.util.jar.JarFile.<init>(JarFile.java:348)\n",
      "\tat java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)\n",
      "\tat java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)\n",
      "\tat java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)\n",
      "\tat java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)\n",
      "\tat java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)\n",
      "\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)\n",
      "\tat org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)\n",
      "\t... 26 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.nio.file.NoSuchFileException: /home/eashwar123/Desktop/sem4/Bigdata/project/venv/lib/python3.10/site-packages/pyspark/jars/hadoop-client-api-3.3.4.jar\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3089)\n\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)\n\tat org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)\n\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1412)\n\tat org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendSparkHadoopConfigs(SparkHadoopUtil.scala:512)\n\tat org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendS3AndSparkHadoopHiveConfigurations(SparkHadoopUtil.scala:443)\n\tat org.apache.spark.deploy.SparkHadoopUtil$.newConfiguration(SparkHadoopUtil.scala:428)\n\tat org.apache.spark.deploy.SparkHadoopUtil.newConfiguration(SparkHadoopUtil.scala:122)\n\tat org.apache.spark.SecurityManager.<init>(SecurityManager.scala:99)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:262)\n\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:196)\n\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:284)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:483)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.nio.file.NoSuchFileException: /home/eashwar123/Desktop/sem4/Bigdata/project/venv/lib/python3.10/site-packages/pyspark/jars/hadoop-client-api-3.3.4.jar\n\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)\n\tat java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)\n\tat java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)\n\tat java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)\n\tat java.base/java.nio.file.Files.readAttributes(Files.java:1764)\n\tat java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1414)\n\tat java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)\n\tat java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)\n\tat java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)\n\tat java.base/java.util.jar.JarFile.<init>(JarFile.java:348)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)\n\tat java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)\n\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)\n\tat org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)\n\t... 26 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MulticlassClassificationEvaluator\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Initialize Spark Session\u001b[39;00m\n\u001b[1;32m     12\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal[*]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAudio Classification with RandomForest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m4g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m4g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Base directory for audio files\u001b[39;00m\n\u001b[1;32m     20\u001b[0m base_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m~/Downloads/data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/sem4/Bigdata/project/venv/lib/python3.10/site-packages/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/Desktop/sem4/Bigdata/project/venv/lib/python3.10/site-packages/pyspark/context.py:515\u001b[0m, in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    511\u001b[0m         return xrange(getStart(split), getStart(split + 1), step)\n\u001b[1;32m    513\u001b[0m     return self.parallelize([], numSlices).mapPartitionsWithIndex(f)\n\u001b[0;32m--> 515\u001b[0m # Make sure we distribute data evenly if it's smaller than self.batchSize\n\u001b[1;32m    516\u001b[0m if \"__len__\" not in dir(c):\n\u001b[1;32m    517\u001b[0m     c = list(c)    # Make it a list so we can compute its length\n",
      "File \u001b[0;32m~/Desktop/sem4/Bigdata/project/venv/lib/python3.10/site-packages/pyspark/context.py:203\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    199\u001b[0m # Reset the SparkConf to the one actually used by the SparkContext in JVM.\n\u001b[1;32m    200\u001b[0m self._conf = SparkConf(_jconf=self._jsc.sc().conf())\n\u001b[1;32m    202\u001b[0m # Create a single Accumulator in Java that we'll send all our updates through;\n\u001b[0;32m--> 203\u001b[0m # they will be passed back to us through a TCP server\n\u001b[1;32m    204\u001b[0m auth_token = self._gateway.gateway_parameters.auth_token\n\u001b[1;32m    205\u001b[0m self._accumulatorServer = accumulators._start_update_server(auth_token)\n",
      "File \u001b[0;32m~/Desktop/sem4/Bigdata/project/venv/lib/python3.10/site-packages/pyspark/context.py:296\u001b[0m, in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_repr_html_\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124m    <div>\u001b[39m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124m        <p><b>SparkContext</b></p>\u001b[39m\n\u001b[1;32m    286\u001b[0m \n\u001b[1;32m    287\u001b[0m \u001b[38;5;124m        <p><a href=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{sc.uiWebUrl}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>Spark UI</a></p>\u001b[39m\n\u001b[1;32m    288\u001b[0m \n\u001b[1;32m    289\u001b[0m \u001b[38;5;124m        <dl>\u001b[39m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124m          <dt>Version</dt>\u001b[39m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124m            <dd><code>v\u001b[39m\u001b[38;5;132;01m{sc.version}\u001b[39;00m\u001b[38;5;124m</code></dd>\u001b[39m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124m          <dt>Master</dt>\u001b[39m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;124m            <dd><code>\u001b[39m\u001b[38;5;132;01m{sc.master}\u001b[39;00m\u001b[38;5;124m</code></dd>\u001b[39m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124m          <dt>AppName</dt>\u001b[39m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124m            <dd><code>\u001b[39m\u001b[38;5;132;01m{sc.appName}\u001b[39;00m\u001b[38;5;124m</code></dd>\u001b[39m\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;124m        </dl>\u001b[39m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124m    </div>\u001b[39m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    299\u001b[0m         sc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    300\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/sem4/Bigdata/project/venv/lib/python3.10/site-packages/pyspark/context.py:421\u001b[0m, in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefaultParallelism\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    420\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;124;03m    Default level of parallelism to use when not given by user (e.g. for\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m    reduce tasks)\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mdefaultParallelism()\n",
      "File \u001b[0;32m~/Desktop/sem4/Bigdata/project/venv/lib/python3.10/site-packages/py4j/java_gateway.py:1587\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to call a package.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1585\u001b[0m new_fqn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fqn \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name\n\u001b[1;32m   1586\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mREFLECTION_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m-> 1587\u001b[0m     proto\u001b[38;5;241m.\u001b[39mREFL_GET_UNKNOWN_SUB_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1588\u001b[0m     new_fqn \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1589\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1590\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1591\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m   1592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n",
      "File \u001b[0;32m~/Desktop/sem4/Bigdata/project/venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/sem4/Bigdata/project/venv/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.nio.file.NoSuchFileException: /home/eashwar123/Desktop/sem4/Bigdata/project/venv/lib/python3.10/site-packages/pyspark/jars/hadoop-client-api-3.3.4.jar\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3089)\n\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)\n\tat org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)\n\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1412)\n\tat org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendSparkHadoopConfigs(SparkHadoopUtil.scala:512)\n\tat org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendS3AndSparkHadoopHiveConfigurations(SparkHadoopUtil.scala:443)\n\tat org.apache.spark.deploy.SparkHadoopUtil$.newConfiguration(SparkHadoopUtil.scala:428)\n\tat org.apache.spark.deploy.SparkHadoopUtil.newConfiguration(SparkHadoopUtil.scala:122)\n\tat org.apache.spark.SecurityManager.<init>(SecurityManager.scala:99)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:262)\n\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:196)\n\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:284)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:483)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.nio.file.NoSuchFileException: /home/eashwar123/Desktop/sem4/Bigdata/project/venv/lib/python3.10/site-packages/pyspark/jars/hadoop-client-api-3.3.4.jar\n\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)\n\tat java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)\n\tat java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)\n\tat java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)\n\tat java.base/java.nio.file.Files.readAttributes(Files.java:1764)\n\tat java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1414)\n\tat java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)\n\tat java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)\n\tat java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)\n\tat java.base/java.util.jar.JarFile.<init>(JarFile.java:348)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)\n\tat java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)\n\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)\n\tat org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)\n\t... 26 more\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Audio Classification with RandomForest\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Base directory for audio files\n",
    "base_dir = os.path.expanduser('~/Downloads/data')\n",
    "directories = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "\n",
    "# Function to extract MFCC features\n",
    "def extract_features(file_path):\n",
    "    audio, sample_rate = librosa.load(file_path, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13)\n",
    "    return mfcc.mean(axis=1).tolist()\n",
    "\n",
    "# Load and preprocess data\n",
    "audio_features = []\n",
    "for directory in directories:\n",
    "    genre = directory\n",
    "    path = os.path.join(base_dir, directory)\n",
    "    audio_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.wav')]\n",
    "    for file in audio_files:\n",
    "        features = extract_features(file)\n",
    "        audio_features.append((genre, file, features))\n",
    "\n",
    "# Create DataFrame\n",
    "df_audio = pd.DataFrame(audio_features, columns=['genre', 'file_path', 'audio_features'])\n",
    "sdf_audio = spark.createDataFrame(df_audio)\n",
    "\n",
    "# Convert array of doubles to Vector\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "sdf_audio = sdf_audio.withColumn(\"features\", list_to_vector_udf(\"audio_features\"))\n",
    "\n",
    "# Index labels\n",
    "indexer = StringIndexer(inputCol=\"genre\", outputCol=\"label\")\n",
    "sdf_audio = indexer.fit(sdf_audio).transform(sdf_audio)\n",
    "\n",
    "# Split the data\n",
    "train_data, test_data = sdf_audio.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Define and train the Random Forest model\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='label', numTrees=20, maxDepth=5, seed=42)\n",
    "model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "f1_score = evaluator_f1.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)\n",
    "\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
