{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bytes' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m udf, rand\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vectors, VectorUDT\n",
      "File \u001b[0;32m~/Desktop/sem4/Bigdata/project/venv/lib/python3.10/site-packages/pyspark/__init__.py:51\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkConf\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrdd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RDD, RDDBarrier\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfiles\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkFiles\n",
      "File \u001b[0;32m~/Desktop/sem4/Bigdata/project/venv/lib/python3.10/site-packages/pyspark/context.py:31\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtempfile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NamedTemporaryFile\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Py4JError\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accumulators\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccumulators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Accumulator\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbroadcast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Broadcast, BroadcastPickleRegistry\n",
      "File \u001b[0;32m~/Desktop/sem4/Bigdata/project/venv/lib/python3.10/site-packages/pyspark/accumulators.py:97\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msocketserver\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mSocketServer\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserializers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_int, PickleSerializer\n\u001b[1;32m    100\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccumulator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccumulatorParam\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    103\u001b[0m pickleSer \u001b[38;5;241m=\u001b[39m PickleSerializer()\n",
      "File \u001b[0;32m~/Desktop/sem4/Bigdata/project/venv/lib/python3.10/site-packages/pyspark/serializers.py:72\u001b[0m\n\u001b[1;32m     69\u001b[0m     protocol \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     70\u001b[0m     xrange \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cloudpickle\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _exception_message\n\u001b[1;32m     76\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPickleSerializer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMarshalSerializer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUTF8Deserializer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/sem4/Bigdata/project/venv/lib/python3.10/site-packages/pyspark/cloudpickle.py:145\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m types\u001b[38;5;241m.\u001b[39mCodeType(\n\u001b[1;32m    127\u001b[0m             co\u001b[38;5;241m.\u001b[39mco_argcount,\n\u001b[1;32m    128\u001b[0m             co\u001b[38;5;241m.\u001b[39mco_kwonlyargcount,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m             (),\n\u001b[1;32m    142\u001b[0m         )\n\u001b[0;32m--> 145\u001b[0m _cell_set_template_code \u001b[38;5;241m=\u001b[39m \u001b[43m_make_cell_set_template_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcell_set\u001b[39m(cell, value):\n\u001b[1;32m    149\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Set the value of a closure cell.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/sem4/Bigdata/project/venv/lib/python3.10/site-packages/pyspark/cloudpickle.py:126\u001b[0m, in \u001b[0;36m_make_cell_set_template_code\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m types\u001b[38;5;241m.\u001b[39mCodeType(\n\u001b[1;32m    110\u001b[0m         co\u001b[38;5;241m.\u001b[39mco_argcount,\n\u001b[1;32m    111\u001b[0m         co\u001b[38;5;241m.\u001b[39mco_nlocals,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m         (),\n\u001b[1;32m    124\u001b[0m     )\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCodeType\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_argcount\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_kwonlyargcount\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_nlocals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_stacksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_flags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_consts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_varnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_firstlineno\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_lnotab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_cellvars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# this is the trickery\u001b[39;49;00m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'bytes' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, rand\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Audio Classification\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Base directory for audio files\n",
    "base_dir = os.path.expanduser('~/Downloads/data')\n",
    "directories = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "\n",
    "# Function to extract MFCC features\n",
    "def extract_features(file_path):\n",
    "    audio, sample_rate = librosa.load(file_path, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13)\n",
    "    return mfcc.mean(axis=1).tolist()\n",
    "\n",
    "# Load and preprocess data\n",
    "audio_features = []\n",
    "for directory in directories:\n",
    "    genre = directory\n",
    "    path = os.path.join(base_dir, directory)\n",
    "    audio_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.wav')]\n",
    "    for file in audio_files:\n",
    "        features = extract_features(file)\n",
    "        audio_features.append((genre, file, features))\n",
    "\n",
    "# Create DataFrame\n",
    "df_audio = pd.DataFrame(audio_features, columns=['genre', 'file_path', 'audio_features'])\n",
    "sdf_audio = spark.createDataFrame(df_audio)\n",
    "\n",
    "# Convert array of doubles to Vector\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "sdf_audio = sdf_audio.withColumn(\"features\", list_to_vector_udf(\"audio_features\"))\n",
    "\n",
    "# Index labels\n",
    "indexer = StringIndexer(inputCol=\"genre\", outputCol=\"label\")\n",
    "sdf_audio = indexer.fit(sdf_audio).transform(sdf_audio)\n",
    "\n",
    "# Split the data\n",
    "train_data, test_data = sdf_audio.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Logistic Regression Model\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label', maxIter=10, family=\"multinomial\")\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "f1_score = evaluator_f1.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)\n",
    "\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
